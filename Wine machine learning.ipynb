{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.mlab as mlab\n",
    "from numpy import average\n",
    "from matplotlib.pyplot import figure\n",
    "import category_encoders as ce\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.tree import export_graphviz\n",
    "from sklearn.metrics import mean_absolute_error as mae\n",
    "import pydot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import CSVs, assign headers and remove quotations from column headers\n",
    "red_wine = pd.read_csv(r'C:\\Users\\Killian\\Projects\\Wine analysis\\Wine data\\winequality-red.csv', sep=';',header=0, engine='python')\n",
    "white_wine = pd.read_csv(r'C:\\Users\\Killian\\Projects\\Wine analysis\\Wine data\\winequality-white.csv', sep=';',header=0, engine='python')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "red_wine['quality_label'] = red_wine['quality'].apply(lambda value: 'low'\n",
    "if value <= 4 else 'medium-low'\n",
    "if value == 5 else 'medium' if value == 6 else 'medium-high' if value == 7 else 'high')\n",
    "\n",
    "white_wine['quality_label'] = white_wine['quality'].apply(lambda value: 'low'\n",
    "if value <= 4 else 'medium-low'\n",
    "if value == 5 else 'medium' if value == 6 else 'medium-high' if value == 7 else 'high')\n",
    "\n",
    "red_wine['quality_label'] = pd.Categorical(red_wine['quality_label'],\n",
    "categories=['low', 'medium-low', 'medium', 'medium-high', 'high'])\n",
    "\n",
    "white_wine['quality_label'] = pd.Categorical(white_wine['quality_label'],\n",
    "categories=['low', 'medium-low', 'medium', 'medium-high', 'high'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a new dataset combining red and white wines\n",
    "red_wine['wine_type'] = 'red'\n",
    "white_wine['wine_type'] = 'white'\n",
    "wines = pd.concat([red_wine, white_wine])\n",
    "#wines['ql_code'] = wines['quality_label']\n",
    "\n",
    "# re-shuffle records just to randomize data points as you may have a problem when fitting machine learning model. Dataset may not be split in a fair way.\n",
    "wines = wines.sample(frac=1, random_state=42).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Skewness</th>\n",
       "      <th>Kurtosis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>fixed acidity</th>\n",
       "      <td>1.72</td>\n",
       "      <td>5.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>volatile acidity</th>\n",
       "      <td>1.50</td>\n",
       "      <td>2.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>citric acid</th>\n",
       "      <td>0.47</td>\n",
       "      <td>2.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>residual sugar</th>\n",
       "      <td>1.44</td>\n",
       "      <td>4.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chlorides</th>\n",
       "      <td>5.40</td>\n",
       "      <td>50.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <td>1.22</td>\n",
       "      <td>7.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <td>-0.00</td>\n",
       "      <td>-0.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>density</th>\n",
       "      <td>0.50</td>\n",
       "      <td>6.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pH</th>\n",
       "      <td>0.39</td>\n",
       "      <td>0.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sulphates</th>\n",
       "      <td>1.80</td>\n",
       "      <td>8.65</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Skewness  Kurtosis\n",
       "fixed acidity             1.72      5.06\n",
       "volatile acidity          1.50      2.83\n",
       "citric acid               0.47      2.40\n",
       "residual sugar            1.44      4.36\n",
       "chlorides                 5.40     50.90\n",
       "free sulfur dioxide       1.22      7.91\n",
       "total sulfur dioxide     -0.00     -0.37\n",
       "density                   0.50      6.61\n",
       "pH                        0.39      0.37\n",
       "sulphates                 1.80      8.65"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Skewness is the degree of distortion from the symmetrical bell curve or the normal distribution. It measures the lack of symmetry in data distribution.\n",
    "# It differentiates extreme values in one versus the other tail. A symmetrical distribution will have a skewness of 0.\n",
    "# If the skewness is between -1 and -0.5(negatively skewed) or between 0.5 and 1(positively skewed), the data are moderately skewed. \n",
    "# If the skewness is less than -1(negatively skewed) or greater than 1(positively skewed), the data are highly skewed.\n",
    "\n",
    "# Kurtosis is all about the tails of the distribution — not the peakedness or flatness. It is used to describe the extreme values\n",
    "# in one versus the other tail. It is actually the measure of outliers present in the distribution.\n",
    "\n",
    "# Mesokurtic: This distribution has kurtosis statistic similar to that of the normal distribution. It means that the extreme values of the \n",
    "# distribution are similar to that of a normal distribution characteristic. This definition is used so that the standard normal distribution has a kurtosis of three.\n",
    "\n",
    "# Leptokurtic (Kurtosis > 3): Distribution is longer, tails are fatter. Peak is higher and sharper than Mesokurtic, which means that data are heavy-tailed or profusion \n",
    "# of outliers. Outliers stretch the horizontal axis of the histogram graph, which makes the bulk of the data appear in a narrow (“skinny”) vertical range, thereby giving \n",
    "# the “skinniness” of a leptokurtic distribution.\n",
    "\n",
    "# Platykurtic: (Kurtosis < 3): Distribution is shorter, tails are thinner than the normal distribution. The peak is lower and broader than Mesokurtic, which means that \n",
    "# data are light-tailed or lack of outliers. The reason for this is because the extreme values are less than that of the normal distribution.\n",
    "\n",
    "\n",
    "wine = []\n",
    "for column in wines.columns[0:-4]:\n",
    "        wine.append([wines[column].astype(float).skew().round(2), wines[column].astype(float).kurt().round(2)])\n",
    "\n",
    "wine_stats = pd.DataFrame(wine, index=[wines.columns[0:-4]], columns=['Skewness', 'Kurtosis'])\n",
    "display(wine_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "#enc = OrdinalEncoder(categories=[['low', 'medium-low', 'medium', 'medium-high', 'high']])\n",
    "\n",
    "X=wines.copy()\n",
    "\n",
    "#wine_features[['ql_code']] = enc.fit_transform(wine_features[['ql_code']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I will keep all features for now. Will remove low correlation features later to see if there is an impact\n",
    "\n",
    "y = X.wine_type\n",
    "y = pd.get_dummies(y)\n",
    "\n",
    "X.drop(['quality_label', 'quality', 'wine_type'],axis=1,inplace=True)\n",
    "\n",
    "X = pd.get_dummies(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar', 'chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'density', 'pH', 'sulphates', 'alcohol']\n"
     ]
    }
   ],
   "source": [
    "y = np.array(y)\n",
    "# Remove the labels from the features\n",
    "# Saving feature names for later use\n",
    "X_list = list(X.columns)\n",
    "# Convert to numpy array\n",
    "X = np.array(X)\n",
    "print(X_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Features Shape: (5197, 11)\n",
      "Training Labels Shape: (5197, 2)\n",
      "Testing Features Shape: (1300, 11)\n",
      "Testing Labels Shape: (1300, 2)\n"
     ]
    }
   ],
   "source": [
    "# Because of the skewedness of the model, with many outliers, it does not seem appropriate to use either normalisation or\n",
    "# standardisation. I will be applying a tree based algorithm which is insensitive to the scale of features as it splits\n",
    "# nodes based on single features. \n",
    "\n",
    "#Below I am testing to make sure the shapes are as predicted (column numbers match)\n",
    "\n",
    "print('Training Features Shape:', X_train.shape)\n",
    "print('Training Labels Shape:', y_train.shape)\n",
    "print('Testing Features Shape:', X_test.shape)\n",
    "print('Testing Labels Shape:', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate model with 1000 decision trees\n",
    "rf = RandomForestRegressor(n_estimators = 1000, random_state = 42)\n",
    "# Train the model on training data\n",
    "rf.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [5197, 1300]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-220-0a9cc85705c3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmae\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# Use the forest's predict method on the test data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#predictions = rf.predict(X_test)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# Calculate the absolute errors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;31m# extra_args > 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_regression.py\u001b[0m in \u001b[0;36mmean_absolute_error\u001b[1;34m(y_true, y_pred, sample_weight, multioutput)\u001b[0m\n\u001b[0;32m    180\u001b[0m     \u001b[1;36m0.85\u001b[0m\u001b[1;33m...\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    181\u001b[0m     \"\"\"\n\u001b[1;32m--> 182\u001b[1;33m     y_type, y_true, y_pred, multioutput = _check_reg_targets(\n\u001b[0m\u001b[0;32m    183\u001b[0m         y_true, y_pred, multioutput)\n\u001b[0;32m    184\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_regression.py\u001b[0m in \u001b[0;36m_check_reg_targets\u001b[1;34m(y_true, y_pred, multioutput, dtype)\u001b[0m\n\u001b[0;32m     86\u001b[0m         \u001b[0mthe\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0margument\u001b[0m \u001b[0mpassed\u001b[0m \u001b[0mto\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m     \"\"\"\n\u001b[1;32m---> 88\u001b[1;33m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     89\u001b[0m     \u001b[0my_true\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    260\u001b[0m     \u001b[0muniques\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    261\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 262\u001b[1;33m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[0m\u001b[0;32m    263\u001b[0m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[0;32m    264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [5197, 1300]"
     ]
    }
   ],
   "source": [
    "mae(y_train, y_test)\n",
    "\n",
    "# Use the forest's predict method on the test data\n",
    "#predictions = rf.predict(X_test)\n",
    "# Calculate the absolute errors\n",
    "#errors = abs(predictions - y_test)\n",
    "# Print out the mean absolute error (mae)\n",
    "#print('Mean Absolute Error:', round(np.mean(errors), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: nan %.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-219-c9106e4a08ae>:2: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  mape = 100 * (errors / y_test)\n",
      "<ipython-input-219-c9106e4a08ae>:2: RuntimeWarning: invalid value encountered in true_divide\n",
      "  mape = 100 * (errors / y_test)\n"
     ]
    }
   ],
   "source": [
    "# Calculate mean absolute percentage error (MAPE)\n",
    "mape = 100 * (errors / y_test)\n",
    "# Calculate and display accuracy\n",
    "accuracy = 100 - np.mean(mape)\n",
    "print('Accuracy:', round(accuracy, 2), '%.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Pull out one tree from the forest\n",
    "tree = rf.estimators_[5]\n",
    "# Export the image to a dot file\n",
    "export_graphviz(tree, out_file = 'tree.dot', feature_names = train_wine_list, rounded = True, precision = 1)\n",
    "# Use dot file to create a graph\n",
    "(graph, ) = pydot.graph_from_dot_file('tree.dot')\n",
    "# Write graph to a png file\n",
    "graph.write_png('tree.png')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ac59ebe37160ed0dfa835113d9b8498d9f09ceb179beaac4002f036b9467c963"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
